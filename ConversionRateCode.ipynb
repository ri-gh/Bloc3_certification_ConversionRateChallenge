{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "print('Set with labels (our train+test) :', data.shape)\n",
    "\n",
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(data.shape[0]))\n",
    "print()\n",
    "print(\"Number of columns : {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*data.isnull().sum()/data.shape[0])\n",
    "\n",
    "#removing outliers\n",
    "data = data[data.loc[:,'age'] < 100] \n",
    "data.describe(include ='all')\n",
    "\n",
    "#repartition des conversion rate par new user:\n",
    "plt.figure(figsize=[12,5])\n",
    "sns.countplot(data['converted'], hue = data['new_user']).set(title='Part of conversion per new user')\n",
    "\n",
    "\n",
    "sns.relplot(data=data, x=\"total_pages_visited\", y=\"converted\")\n",
    "\n",
    "# Univariate analysis\n",
    "#Distribution of quantitative variables\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Distribution of each numeric variable\n",
    "num_features = ['age','total_pages_visited']\n",
    "fig1 = make_subplots(rows = len(num_features), cols = 1, subplot_titles = num_features)\n",
    "for i in range(len(num_features)):\n",
    "    fig1.add_trace(\n",
    "        go.Histogram(\n",
    "            x = data[num_features[i]], nbinsx = 4),\n",
    "        row = i + 1,\n",
    "        col = 1)\n",
    "fig1.update_layout(\n",
    "        title = go.layout.Title(text = \"Distribution of quantitative variables\", x = 0.5), showlegend = False, \n",
    "            autosize=False, height=500)\n",
    "fig1.show()\n",
    "\n",
    "\n",
    "# Univariate analysis\n",
    "\n",
    "# Barplot of each qualitative variable\n",
    "\n",
    "cat_features = ['country','new_user','source','converted']\n",
    "fig2 = make_subplots(rows = len(cat_features), cols = 1, subplot_titles = cat_features)\n",
    "for i in range(len(cat_features)):\n",
    "    \n",
    "    x_coords = data[cat_features[i]].value_counts().index.tolist()\n",
    "    y_coords = data[cat_features[i]].value_counts().tolist()\n",
    "    \n",
    "    fig2.add_trace(\n",
    "        go.Bar(\n",
    "            x = x_coords,\n",
    "            y = y_coords),\n",
    "        row = i + 1,\n",
    "        col = 1)\n",
    "fig2.update_layout(\n",
    "        title = go.layout.Title(text = \"Barplot of qualitative variables\", x = 0.5), showlegend = False, \n",
    "            autosize=False, height=500)\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_annotated_heatmap(corr_matrix.values,\n",
    "                                  x = corr_matrix.columns.values.tolist(),\n",
    "                                  y = corr_matrix.index.values.tolist())\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "#Let's try a first basic model : simple logistic regression with only one feature.\n",
    "#We choose the total_pages_visited because we just noticed that it is strongly correlated to the converted.\n",
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "features_list = [\"total_pages_visited\"]\n",
    "target_variable = \"converted\"\n",
    "\n",
    "X = data.loc[:,features_list]\n",
    "Y = data.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())\n",
    "\n",
    "\n",
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \n",
    "                                                    random_state=0,stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.tolist()\n",
    "Y_test = Y_test.tolist()\n",
    "print(\"...Done\")\n",
    "\n",
    "print(X_train[0:5,:])\n",
    "print(X_test[0:2,:])\n",
    "print()\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:2])\n",
    "\n",
    "# Standardizing numerical features\n",
    "print(\"Standardizing numerical features...\")\n",
    "print()\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "\n",
    "# Train model with a logistic regression:\n",
    "print(\"Train model...\")\n",
    "classifier  = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()\n",
    "\n",
    "\n",
    "# Standardizing numerical features on Test set:\n",
    "print(\"Standardizing numerical features...\")\n",
    "print()\n",
    "print(X_test[0:5,:])\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_test[0:5,:])\n",
    "\n",
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()\n",
    "\n",
    "\n",
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(Y_train, Y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "#le modèle a une bonne accuracy (taux de prédiction exacte très correct) \n",
    "# mais le F1 score (mesure de tous les cas positifs) n'est pas forcément bon même si pas si mauvais il fait quelques erreurs quand même (pas assez de features),\n",
    "#  modèle pas assez complexe\n",
    "\n",
    "# Visualize confusion matrices on train set\n",
    "_ , ax = plt.subplots() # Get subplot from matplotlib\n",
    "ax.set(title=\"Confusion Matrix on Train set\") # Set a title that we will add into ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(classifier, X_train, Y_train, ax=ax) # ConfusionMatrixDisplay from sklearn\n",
    "plt.show()\n",
    "\n",
    "# Visualize confusion matrices on test set\n",
    "_ , ax = plt.subplots() # Get subplot from matplotlib\n",
    "ax.set(title=\"Confusion Matrix on Test set\") # Set a title that we will add into ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(classifier, X_test, Y_test, ax=ax) # ConfusionMatrixDisplay from sklearn\n",
    "plt.show()\n",
    "\n",
    "#Train a multivariate model by adding all the others features:\n",
    "#removing outliers\n",
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "data = data[data.loc[:,'age'] < 100] \n",
    "\n",
    "# Separate target variable Y from features X , on prend pas country:\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "features_list = [ \"country\",\"age\", \"new_user\",\"source\",\"total_pages_visited\"]\n",
    "target_variable = \"converted\"\n",
    "\n",
    "X = data.loc[:,features_list]\n",
    "Y = data.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=0,stratify=Y)\n",
    "\n",
    "\n",
    "numeric_features = [1, 4] # Choose which column index we are going to scale\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [0, 2, 3] # Choose which column index we are going to encode\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),    \n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done.\")\n",
    "print(\"#### X_train AFTER preprocessing ####\")\n",
    "print(X_train[0:5,:]) # print first 5 rows (not using iloc since now X_train became a numpy array)\n",
    "print()\n",
    "\n",
    "# Training model \n",
    "print(\"Training model...\")\n",
    "classifier =LogisticRegression() # Instanciate model \n",
    "classifier.fit(X_train, y_train) # Fit model\n",
    "print(\"...Done.\")\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on train set...\")\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print(\"#### First five predictions on TRAIN set ####\")\n",
    "print(y_train_pred[0:5])\n",
    "\n",
    "\n",
    "### Test pipeline ###\n",
    "print(\"--- Test pipeline ---\") \n",
    "\n",
    "# Encoding categorical features and standardizing numeric features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done.\")\n",
    "\n",
    "print(\"#### X_test AFTER preprocessing ####\")\n",
    "print(X_test[0:5,:])\n",
    "print()\n",
    "\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on test set...\")\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print(\"#### First five predictions on TEST set ####\")\n",
    "print(y_train_pred[0:5])\n",
    "\n",
    "\n",
    "### Assessment of performances ###\n",
    "print(\"--- Assessment of performances ---\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = plot_confusion_matrix(classifier, X_train, y_train)\n",
    "cm.ax_.set_title(\"Confusion matrix on train set \") # Simply to set a title\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on train set : \", classifier.score(X_train, y_train))\n",
    "\n",
    "\n",
    "cm = plot_confusion_matrix(classifier, X_test, y_test)\n",
    "cm.ax_.set_title(\"Confusion matrix on test set \")\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on test set : \", classifier.score(X_test, y_test))\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(y_train, y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "#F1 & accuracy scores improved on train & test set\n",
    "\n",
    "\n",
    "# Check coefficients \n",
    "\n",
    "print(\"coefficients are: \", classifier.coef_) \n",
    "\n",
    "# Access transformers from feature_encoder\n",
    "print(\"All transformers are: \", featureencoder.transformers_)\n",
    "\n",
    "# Access one specific transformer\n",
    "print(\"One Hot Encoder transformer is: \", featureencoder.transformers_[0][1])\n",
    "\n",
    "# Print categories\n",
    "categorical_column_names = featureencoder.transformers_[0][1].categories_\n",
    "categorical_column_names = np.concatenate(categorical_column_names).ravel()\n",
    "print(\"Categorical columns are: \", categorical_column_names)\n",
    "\n",
    "# Print numerical columns\n",
    "numerical_column_names = X.iloc[:, numeric_features].columns # using the .columns attribute gives us the name of the column \n",
    "print(\"numerical columns are: \", numerical_column_names)\n",
    "\n",
    "# Append all columns \n",
    "all_column_names = np.append(categorical_column_names, numerical_column_names)\n",
    "print(\"All column names are: \", all_column_names)\n",
    "\n",
    "# Feature importance \n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature_names\": all_column_names,\n",
    "    \"coefficients\":classifier.coef_.squeeze() # CAREFUL HERE. We need to access first index of our list because \n",
    "                                            # Data need to be 1 dimensional\n",
    "                                            # That's what .squeeze()\n",
    "})\n",
    "\n",
    "feature_importance \n",
    "\n",
    "\n",
    "# Set coefficient to absolute values to rank features\n",
    "feature_importance[\"coefficients\"] = feature_importance[\"coefficients\"].abs()\n",
    "\n",
    "# Visualize ranked features using seaborn\n",
    "sns.catplot(x=\"feature_names\", \n",
    "            y=\"coefficients\", \n",
    "            data=feature_importance.sort_values(by=\"coefficients\", ascending=False), \n",
    "            kind=\"bar\",\n",
    "            aspect=3) # Resize graph\n",
    "\n",
    "\n",
    " #le modèle a une très bonne accuracy (taux de prédiction exacte très correct)\n",
    " # le F1 score s'est amélioré en ajoutant des features (le modele a réussi à faire moins d'erreurs )\n",
    " # F1 se rapproche de 1        \n",
    "\n",
    "\n",
    "# Concatenate our train and test set to train your best classifier on all data with labels\n",
    "X = np.append(X_train,X_test,axis=0)\n",
    "Y = np.append(y_train,y_test)\n",
    "\n",
    "classifier.fit(X,Y)\n",
    "\n",
    "# Read data without labels\n",
    "data_without_labels = pd.read_csv('conversion_data_test.csv')\n",
    "print('Prediction set (without labels) :', data_without_labels.shape)\n",
    "\n",
    "# Warning : check consistency of features_list (must be the same than the features \n",
    "# used by your best classifier)\n",
    "features_list = [\"country\",\"age\", \"new_user\",\"source\",\"total_pages_visited\"]\n",
    "X_without_labels = data_without_labels.loc[:, features_list]\n",
    "\n",
    "# 'new_user' is a categorical feature in fact 'yes' or 'no' not a numerical one\n",
    "#so we amend the list of categorical features to:\n",
    "categorical_features = ['country','source', 'new_user']\n",
    "numeric_features = ['total_pages_visited', 'age']\n",
    "\n",
    "\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "\n",
    "X_without_labels = featureencoder.transform(X_without_labels)\n",
    "print(\"...Done\")\n",
    "print(X_without_labels[0:5,:])\n",
    "\n",
    "# Make predictions and dump to file\n",
    "data = {\n",
    "    'converted': classifier.predict(X_without_labels)\n",
    "}\n",
    "\n",
    "Y_predictions = pd.DataFrame(columns=['converted'],data=data)\n",
    "Y_predictions.to_csv('conversion_data_test_predictions_RICHARDGH-M2.csv', index=False)\n",
    "\n",
    "#On va essayer un modele de Régression logistique sans le critère 'total_pages_visited':\n",
    "\n",
    "\n",
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "print('Set with labels (our train+test) :', data.shape)\n",
    "#removing outliers\n",
    "data = data[data.loc[:,'age'] < 100] \n",
    "\n",
    "# Separate target variable Y from features X , on prend pas country:\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "features_list = [ \"country\",\"age\", \"new_user\",\"source\"]\n",
    "target_variable = \"converted\"\n",
    "\n",
    "X = data.loc[:,features_list]\n",
    "Y = data.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=0,stratify=Y)\n",
    "\n",
    "\n",
    "numeric_features = [1] # Choose which column index we are going to scale\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [0, 2, 3] # Choose which column index we are going to encode\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),    \n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done.\")\n",
    "print(\"#### X_train AFTER preprocessing ####\")\n",
    "print(X_train[0:5,:]) # print first 5 rows (not using iloc since now X_train became a numpy array)\n",
    "print()\n",
    "\n",
    "# Training model \n",
    "print(\"Training model...\")\n",
    "classifier =LogisticRegression() # Instanciate model \n",
    "classifier.fit(X_train, y_train) # Fit model\n",
    "print(\"...Done.\")\n",
    "\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on train set...\")\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print(\"#### First five predictions on TRAIN set ####\")\n",
    "print(y_train_pred[0:5])\n",
    "\n",
    "\n",
    "### Test pipeline ###\n",
    "print(\"--- Test pipeline ---\") \n",
    "\n",
    "# Encoding categorical features and standardizing numeric features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done.\")\n",
    "\n",
    "print(\"#### X_test AFTER preprocessing ####\")\n",
    "print(X_test[0:5,:])\n",
    "print()\n",
    "\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on test set...\")\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print(\"#### First five predictions on TEST set ####\")\n",
    "print(y_train_pred[0:5])\n",
    "\n",
    "\n",
    "\n",
    "### Assessment of performances ###\n",
    "print(\"--- Assessment of performances ---\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = plot_confusion_matrix(classifier, X_train, y_train)\n",
    "cm.ax_.set_title(\"Confusion matrix on train set \") # Simply to set a title\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on train set : \", classifier.score(X_train, y_train))\n",
    "\n",
    "\n",
    "cm = plot_confusion_matrix(classifier, X_test, y_test)\n",
    "cm.ax_.set_title(\"Confusion matrix on test set \")\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on test set : \", classifier.score(X_test, y_test))\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(y_train, y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "\n",
    "# Check coefficients \n",
    "\n",
    "print(\"coefficients are: \", classifier.coef_) \n",
    "print(\"Constant is: \", classifier.intercept_)\n",
    "\n",
    "# Access transformers from feature_encoder\n",
    "print(\"All transformers are: \", featureencoder.transformers_)\n",
    "\n",
    "# Access one specific transformer\n",
    "print(\"One Hot Encoder transformer is: \", featureencoder.transformers_[0][1])\n",
    "\n",
    "# Print categories\n",
    "categorical_column_names = featureencoder.transformers_[0][1].categories_\n",
    "categorical_column_names = np.concatenate(categorical_column_names).ravel()\n",
    "print(\"Categorical columns are: \", categorical_column_names)\n",
    "\n",
    "# Print numerical columns\n",
    "numerical_column_names = X.iloc[:, numeric_features].columns # using the .columns attribute gives us the name of the column \n",
    "print(\"numerical columns are: \", numerical_column_names)\n",
    "\n",
    "# Append all columns \n",
    "all_column_names = np.append(categorical_column_names, numerical_column_names)\n",
    "print(\"All column names are: \", all_column_names)\n",
    "\n",
    "# Feature importance \n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature_names\": all_column_names,\n",
    "    \"coefficients\":classifier.coef_.squeeze() # CAREFUL HERE. We need to access first index of our list because \n",
    "                                            # Data need to be 1 dimensional\n",
    "                                            # That's what .squeeze()\n",
    "})\n",
    "\n",
    "feature_importance \n",
    "\n",
    "\n",
    "\n",
    "# Set coefficient to absolute values to rank features\n",
    "feature_importance[\"coefficients\"] = feature_importance[\"coefficients\"].abs()\n",
    "\n",
    "# Visualize ranked features using seaborn\n",
    "sns.catplot(x=\"feature_names\", \n",
    "            y=\"coefficients\", \n",
    "            data=feature_importance.sort_values(by=\"coefficients\", ascending=False), \n",
    "            kind=\"bar\",\n",
    "            aspect=3) # Resize graph\n",
    "\n",
    "\n",
    "#Le modèle a perdu en accuracy le taux de prédication correct a baissé mais surtout le F1 score est tombé à 0 sur le train et le test \n",
    "# car il n'a pu prédire aucun positif du tout (conversion rate = \"1\") \n",
    "# sans la variable 'total_pages_visited'\n",
    "\n",
    "\n",
    "#DECISION TREE MODEL:\n",
    "\n",
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "print('Set with labels (our train+test) :', data.shape)\n",
    "#removing outliers\n",
    "data = data[data.loc[:,'age'] < 100]\n",
    "\n",
    "# Separate target variable Y from features X:\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "features_list = [ \"country\",\"age\", \"new_user\",\"source\",\"total_pages_visited\"]\n",
    "target_variable = \"converted\"\n",
    "\n",
    "X = data.loc[:,features_list]\n",
    "Y = data.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=0,stratify=Y)\n",
    "\n",
    "numeric_features = [1,4] # Choose which column index we are going to scale\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [0, 2, 3] # Choose which column index we are going to encode\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
    "featureencoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),    \n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done.\")\n",
    "print(\"#### X_train AFTER preprocessing ####\")\n",
    "print(X_train[0:5,:]) # print first 5 rows (not using iloc since now X_train became a numpy array)\n",
    "print()\n",
    "\n",
    "\n",
    "# Training model\n",
    "print(\"Training model...\")\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "print(\"...Done.\")\n",
    "\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on train set...\")\n",
    "y_train_pred = DT.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "\n",
    "### Test pipeline ###\n",
    "print(\"--- Test pipeline ---\") \n",
    "\n",
    "# Encoding categorical features and standardizing numeric features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done.\")\n",
    "\n",
    "print(\"#### X_test AFTER preprocessing ####\")\n",
    "print(X_test[0:5,:])\n",
    "print()\n",
    "\n",
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "y_test_pred = DT.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "\n",
    "## Assessment of performances ###\n",
    "print(\"--- Assessment of performances ---\")\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = plot_confusion_matrix(DT, X_train, y_train)\n",
    "cm.ax_.set_title(\"Confusion matrix on train set \") # Simply to set a title\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on train set : \", DT.score(X_train, y_train))\n",
    "\n",
    "\n",
    "cm = plot_confusion_matrix(DT, X_test, y_test)\n",
    "cm.ax_.set_title(\"Confusion matrix on test set \")\n",
    "plt.show() # Show graph\n",
    "print(\"accuracy-score on test set : \", DT.score(X_test, y_test))\n",
    "print('-----')\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(y_train, y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "\n",
    "# Check coefficients \n",
    "\n",
    "print(\"coefficients are: \", DT.feature_importances_) \n",
    "\n",
    "# Access transformers from feature_encoder\n",
    "print(\"All transformers are: \", featureencoder.transformers_)\n",
    "\n",
    "# Access one specific transformer\n",
    "print(\"One Hot Encoder transformer is: \", featureencoder.transformers_[0][1])\n",
    "\n",
    "# Print categories\n",
    "categorical_column_names = featureencoder.transformers_[0][1].categories_\n",
    "categorical_column_names = np.concatenate(categorical_column_names).ravel()\n",
    "print(\"Categorical columns are: \", categorical_column_names)\n",
    "\n",
    "# Print numerical columns\n",
    "numerical_column_names = X.iloc[:, numeric_features].columns # using the .columns attribute gives us the name of the column \n",
    "print(\"numerical columns are: \", numerical_column_names)\n",
    "\n",
    "# Append all columns \n",
    "all_column_names = np.append(categorical_column_names, numerical_column_names)\n",
    "print(\"All column names are: \", all_column_names)\n",
    "\n",
    "\n",
    "# Feature importance \n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature_names\": all_column_names,\n",
    "    \"coefficients\": DT.feature_importances_\n",
    "                                        \n",
    "})\n",
    "\n",
    "feature_importance\n",
    "\n",
    "\n",
    "# Visualize ranked features using seaborn\n",
    "sns.catplot(x=\"feature_names\", \n",
    "            y=\"coefficients\", \n",
    "            data=feature_importance.sort_values(by=\"coefficients\", ascending=False), \n",
    "            kind=\"bar\",\n",
    "            aspect=3) # Resize graph\n",
    "\n",
    "\n",
    "#Le modèle conserve une très bonne accuracy et améliore son F1 score sur le train set, cependant il diminue sur le Test Set \n",
    "# The most important feature to subscribe to the newsletter is the total_pages_visited\n",
    "\n",
    "#PERFORM GRID SEARCH , to get the best parameter for our model:\n",
    "\n",
    "# Perform Grid Search on model to get the best parameter for our model:\n",
    "print(\"Grid search...\")\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [4, 6, 8, 10], #nombre de couches maximales pour le DT\n",
    "    'min_samples_leaf': [1, 2, 5], #specifies the minimum number of samples required to be at a leaf node\n",
    "    'min_samples_split': [2, 4, 8] #min_samples_split specifies the minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "gridsearch = GridSearchCV(DT, param_grid = params, cv = 3) # cv : the number of folds to be used for CV , on divise les datas en 3\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "y_train_pred = gridsearch.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(y_train_pred)\n",
    "print()\n",
    "\n",
    "\n",
    "# Predictions on test set\n",
    "print(\"Predictions on training set...\")\n",
    "y_test_pred = gridsearch.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(y_test_pred)\n",
    "print()\n",
    "\n",
    "# Print scores\n",
    "print(\"accuracy on training set : \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"accuracy on test set : \", accuracy_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "print(\"f1-score on training set : \", f1_score(y_train, y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "\n",
    "#We can see that the score decreased on train set \n",
    "# but increased on the test set after the grid search (pick the best possible hyper-parameters)\n",
    "# on DT hyperparameter optimization worked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#our best model is the logistic regression with alle features to predict\n",
    "\n",
    "#Notre meilleur modèle est la régression logistique avec toutes les features pour faire la prédiction, on voit que le taux de conversion dépend pour beaucoup du nombre de pages visitées, Le modele de régression logistique \n",
    "#avec toutes les features en variables explicatives donne les meilleurs scores.\n",
    "## Bilan:\n",
    "\n",
    "#Notre meilleur modèle est la régression logistique avec toutes les features pour faire la prédiction,\n",
    "#on voit que le taux de conversion dépend pour beaucoup du nombre de pages visitées,\n",
    "#Le modele de régression logistique avec toutes les features en variables explicatives donne les meilleurs scores.\n",
    "\n",
    "#Afin d'améliorer le conversion rate à la souscription à la newsletter ,il faudrait que la page d'accueil soit plus conviviale\n",
    "#et propose des liens plus visibles pour visiter d'autres pages et non une page d'accueil qui met en avant la souscription directement,\n",
    "#ce qui pourrait pousser l'utilisateur à souscrire plus 'facilement' après avoir consulté plus de pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
